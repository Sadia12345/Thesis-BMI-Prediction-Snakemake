\chapter{Methodology}
\label{ch:methods}

\section{Dataset: The Metalog Cohort}
This study utilizes the \texttt{metalog} dataset, a large-scale collection of human gut metagenomes. The raw data consists of taxonomic abundance profiles derived from shotgun metagenomic sequencing. The target variable for prediction is Body Mass Index (BMI).
\begin{itemize}
    \item \textbf{Total Samples:} 18,024
    \item \textbf{Initial Features:} 6,339 species
\end{itemize}

\section{Preprocessing and Feature Selection}
To address the high dimensionality and memory constraints of local execution, we implemented a strict prevalence filter. Species present in less than 1\% of samples were removed.
\begin{equation}
    Keep_i \iff \frac{\sum_{j=1}^{N} \mathbb{I}(Abundance_{ij} > 0)}{N} \ge 0.01
\end{equation}
This step reduced the feature space from ~6,300 to ~1,500, significantly lowering the RAM footprint (see Figure \ref{fig:feature_reduction}).

\section{The Snakemake Workflow}
We developed a reproducible pipeline using Snakemake. The workflow consists of the following rules:
\begin{enumerate}
    \item \texttt{preprocess\_data}: Data cleaning, normalization, and correlation filtering.
    \item \texttt{run\_ml}: Training L2-regularized Generalized Linear Models (GLM/Lasso) with 100 random seeds (cross-validation).
    \item \texttt{plot\_performance}: Aggregating metrics (RMSE, $R^2$) and visualizing results.
\end{enumerate}

\section{Scaling and Saturation Experiment}
To characterize the "learning curve" of the model, we designed a Saturation Study. We generated random subsets of the processed data at logarithmic intervals:
\begin{itemize}
    \item $N = 1,000$
    \item $N = 2,000$
    \item $N = 5,000$
    \item $N = 10,000$
\end{itemize}
The full pipeline was executed for each subset, and performance metrics were compared to identify the point of diminishing returns.
